{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List\n",
    "import tiktoken\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### 简介\n",
    "Tokenization 的主要目的是将文本分解成更小的单位(Tokens)，减小模型输入数据的内在结构复杂度(从句子变为单词序列)，从而简化模型训练的难度。同时将字符的序列转化为Token序号的序列，便于模型输入。\n",
    "\n",
    "Tokenization 首先确定语言的词表划分粒度，一般可分为：\n",
    "* 字符级：将文本分解为字符。\n",
    "* 单词级：将文本分解为单词。\n",
    "* 子词级：将单词进一步分解为更小的有意义单元（如前缀、后缀）。\n",
    "\n",
    "之后使用预定义的规则来识别 tokens, 或使用统计或机器学习技术来识别最优的 token 切分方式。例如，BPE（Byte Pair Encoding）或 SentencePiece。\n",
    "\n",
    "最后实现一组文本序列和Tokens序列之间相互转化的函数，即可完成Tokenization部分。\n",
    "\n",
    "### 实验要求\n",
    "\n",
    "1. 实现字符级切分的简单tokenizer， 由 字符表， 字符到token的 encoder()函数 和 token到字符的 decoder() 函数组成。\n",
    "2. 调用 现有的tokenizer实现，比如openai 的tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataPath:str\n",
    "        ):\n",
    "        with open(dataPath,\"r\",encoding=\"utf-8\") as f:\n",
    "            self.dataset = f.read()\n",
    "        self.generate_vocabulary()\n",
    "\n",
    "    def generate_vocabulary(\n",
    "        self,\n",
    "        ):\n",
    "        # self.char2index = {\n",
    "        #     'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'H':8, 'I':9, 'J':10,\n",
    "        #     'K':11, 'L':12, 'M':13, 'N':14, 'O':15, 'P':16, 'Q':17, 'R':18, 'S':19,\n",
    "        #     'T':20, 'U':21, 'V':22, 'W':23, 'X':24, 'Y':25, 'Z':26, 'a':27, 'b':28,\n",
    "        #     'c':29, 'd':30, 'e':31, 'f':32, 'g':33, 'h':34, 'i':35, 'j':36, 'k':37,\n",
    "        #     'l':38, 'm':39, 'n':40, 'o':41, 'p':42, 'q':43, 'r':44, 's':45, 't':46,\n",
    "        #     'u':47, 'v':48, 'w':49, 'x':50, 'y':51, 'z':52, ' ':53, ',':54, '.':55,\n",
    "        #     '!':56, '?':57, ':':58, ';':59, \"'\":60, '\"':61, '0':62, '1':63, '2':64,\n",
    "        #     }\n",
    "        # self.index2char = {1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F', 7: 'G', 8: 'H', 9: 'I', 10: 'J', 11: 'K', 12: 'L', 13: 'M', 14: 'N', 15: 'O', 16: 'P', 17: 'Q', 18: 'R', 19: 'S', 20: 'T', 21: 'U', 22: 'V', 23: 'W', 24: 'X', 25: 'Y', 26: 'Z', 27: ' ', 28: 'a', 29: 'b', 30: 'c', 31: 'd', 32: 'e', 33: 'f', 34: 'g', 35: 'h', 36: 'i', 37: 'j', 38: 'k', 39: 'l', 40: 'm', 41: 'n', 42: 'o', 43: 'p', 44: 'q', 45: 'r', 46: 's', 47: 't', 48: 'u', 49: 'v', 50: 'w', 51: 'x', 52: 'y', 53: 'z'}\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        sentence : str,\n",
    "        ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        例子, 假设A-Z 对应的token是1-26, 句子开始，结束符号的token是0。\n",
    "        input  : \"ABCD\"\n",
    "        output : Tensor([0,1,2,3]) \n",
    "        注意: 为了后续实验方便，输出Tensor的数据类型dtype 为torch.long。\n",
    "        \"\"\"\n",
    "        # for char in sentence:\n",
    "        #     if char not in self.char2index:\n",
    "        #         raise ValueError(\"Invalid character in sentence\")\n",
    "        ans = torch.tensor([ord(char) for char in sentence],dtype=torch.long)\n",
    "        ans = torch.cat([torch.tensor([0],dtype=torch.long),ans])\n",
    "        return ans\n",
    "    \n",
    "    def decode(\n",
    "        self,\n",
    "        tokens : torch.Tensor,\n",
    "        ) -> str:\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        例子, 假设A-Z 对应的token是1-26, 句子开始，结束符号的token是0。\n",
    "        input : Tensor([0,1,2,3]) \n",
    "        output : \"ABCD\"\n",
    "        \"\"\"\n",
    "        ans = \"\"\n",
    "        for token in tokens:\n",
    "            if token == 0:\n",
    "                continue\n",
    "            ans += chr(token)\n",
    "        return ans\n",
    "\n",
    "# enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "# print(enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 dataloader 和 dataset\n",
    "\n",
    "为了高效加载数据，我们需要把输入文件接入 PyTorch 的数据加载器中。在这里我们定义 `ShakespeareDataset` 类用于加载数据集，用 PyTorch 的 `DataLoader` 类来实现数据加载。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, filepath, tokenizer, chunk_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        self.encoded = self.tokenizer.encode(text)\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded) - self.chunk_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: 提取一段文本(长度为 chunk_size）作为输入，以及这段文本的每一个字符的下一个字符作为标签\n",
    "        # example(not correspond to real text): chunk = tensor([ 0, 20, 49, 58, 59])\n",
    "        #         label = tensor([20, 49, 58, 59, 19])\n",
    "        # decoded chunk: \"The \"\n",
    "        # decoded label: \"he T\"\n",
    "        chunk = self.encoded[idx:idx+self.chunk_size]\n",
    "        label = self.encoded[idx+1:idx+self.chunk_size+1]\n",
    "        return chunk, label\n",
    "\n",
    "tokenizer = Tokenizer(dataPath=\"input.txt\")\n",
    "\n",
    "def create_dataloader(filepath, tokenizer, chunk_size, batch_size, shuffle=True):\n",
    "    dataset = ShakespeareDataset(filepath, tokenizer, chunk_size)\n",
    "    train_dataset,val_dataset = torch.utils.data.random_split(dataset,[int(len(dataset)*0.8),len(dataset)-int(len(dataset)*0.8)])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "train_dataloader,val_dataloader = create_dataloader('input.txt', tokenizer, chunk_size=200, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意力的计算公式为：\n",
    "$$\n",
    "Head = Attention(x)=Softmax(M\\cdot QK^T)V\\\\\n",
    "Q=xW_{q},K=xW_{k}, V=xW_{v}\n",
    "$$\n",
    "这里实现的一些数学技巧可以参见attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadAttention(nn.Module):\n",
    "    def __init__(self, seq_len:int, embed_size:int, hidden_size:int):\n",
    "        super().__init__()\n",
    "        # embed_size: dimension for input embedding vector\n",
    "        # hidden_size: dimension for hidden vector. eg. x:(..., embed_size) --to_q--> query_vector:(..., hidden_size)\n",
    "\n",
    "        # a triangular bool matrix for mask\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(seq_len, seq_len)))\n",
    "        \n",
    "        # TODO: init three matrix, to_q, to_k, to_v.\n",
    "        self.to_q = nn.Linear(embed_size, hidden_size, bias=False) # hidden_size ?\n",
    "        self.to_k = nn.Linear(embed_size, hidden_size, bias=False)\n",
    "        self.to_v = nn.Linear(embed_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size)\n",
    "        # return (batch_size, seq_len, hidden_size)\n",
    "        # TODO: implement the attention mechanism\n",
    "        B, T, C = inputs.shape\n",
    "        # 1. get q, k, v\n",
    "        q = self.to_q(inputs) # (batch_size, seq_len, hidden_size)\n",
    "        k = self.to_k(inputs)\n",
    "        v = self.to_v(inputs)\n",
    "        # 2. calculate attention score\n",
    "        Score = q @ k.transpose(-1,-2)\n",
    "        Score = Score / (C ** 0.5)\n",
    "        # 3. calculate attention score with mask\n",
    "        Score = Score.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        # 4. calculate softmax\n",
    "        Score = F.softmax(Score, dim=-1)\n",
    "        # 5. calculate output\n",
    "        output = Score @ v\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer中使用的注意力机制时会使用多个注意力头，期望每个注意力头能够注意到不同的信息。\n",
    "所以实际公式需要修改如下\n",
    "$$\n",
    "MultiHeadAttention(x)=[Head_0, Head_1,...,Head_h]W_o\\\\\n",
    "Head_i = Attention(x)=Softmax(M\\cdot Q_iK_i^T)V_i\\\\\n",
    "Q_i=xW_{iq},K=xW_{ik}, V=xW_{iv}\n",
    "$$\n",
    "在搭建网络的过程中，同学们可能会用到nn.ModuleList这个库，每个$Head_i$的计算可以直接使用上面已经实现的单头注意力计算。\n",
    "最后对于这些注意力头再使用一个简单的线性层/矩阵$W_o$汇总信息即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # MultiHeadAttention is consist of many HeadAttention output.\n",
    "    # concat all this head attention output o_i, then merge them with a projection matrix W_o, as [o_1, o_2, ...] x W_o\n",
    "    # The reason for using multi-head attention is that we want each head to be able to extract different features\n",
    "    def __init__(self, n_heads:int, head_size:int, seq_len:int, embed_size:int):\n",
    "        # n_heads is the number of head attention\n",
    "        # head_size is the hidden_size in each HeadAttention\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        #TODO: implement heads and projection\n",
    "        self.heads = nn.ModuleList([HeadAttention(seq_len, embed_size, head_size) for _ in range(n_heads)])\n",
    "        self.projection = nn.Linear(embed_size, embed_size, bias=False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size), make sure embed_size=n_heads x head_size\n",
    "        # return: (batch_size, seq_len, embed_size)\n",
    "        # TODO:\n",
    "        outputs = [head(inputs) for head in self.heads]\n",
    "        outputs = torch.cat(outputs, dim=-1)\n",
    "        outputs = self.projection(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 专家网络 Expert\n",
    "\n",
    "Expert即为标准Transformer中的FeedForward模块。\n",
    "\n",
    "在经过MultiHeadAttention 模块后，seq_len中的每一个Embedding都对应了前文信息的加权求和。在经过FeedForward模块时，模型对每一个位置的Embedding进行了两次线性变换和一次非线性变换，可以视为对当前语境下的信息进行加工。知识编辑的一些研究表明，FeedForword 模块参数包含了大量的事实性知识。\n",
    "\n",
    "一个直观的想法是，类比于MultiHeadAttention，我们在每一层训练多个FeedForward模块，对于不同位置的Embedding使用不同的FeedForward模块处理对应的信息。就好像每层有多个Expert,每个Expert都负责处理一类数据的深加工，因此我们称FeedForward为Expert。\n",
    "\n",
    "实现方面:\n",
    "\n",
    "FeedForward层由两层简单的线性层组成，对于一个(batch_size, seq_len, embed_size)输入的向量x\n",
    "只在最后一个维度上进行计算，以实现词的特征维度上的交互(注意力机制是词之间的交互)。\n",
    "其首先用一个线性层将x最后一维扩大至原先4倍，然后继续用一个线性层还原回原先的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, embed_size:int):\n",
    "        super().__init__()\n",
    "        #TODO: init two linear layer\n",
    "        self.linear1 = nn.Linear(embed_size, 4*embed_size)\n",
    "        self.relu = nn.ReLU() # ?\n",
    "        self.linear2 = nn.Linear(4*embed_size, embed_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: (batch_size, seq_len, embed_size)\n",
    "        # -> mid: (batch_size, seq_len, 4 x embed_size)\n",
    "        # -> outputs: (batch_size, seq_len, embed_size)\n",
    "        return self.linear2(self.relu(self.linear1(inputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选通网络 TopkRouter\n",
    "\n",
    "在实现了单个Expert后，我们要设计一个选通网络决策每个Embedding要使用那个Expert计算\n",
    "\n",
    "\n",
    "### 为了说明选通网络的实现方式，我们定义一下记号：\n",
    "\n",
    "inputs.shape = [batch_size, seq_len, embed_size] = [1, 8, 16] \n",
    "\n",
    "即输入有batch_size=1个数据点，该数据有seq_len长度的context，即包含seq_len=8个Embedding，每个Embedding长度为embed_dim=16。\n",
    "\n",
    "记 num_expert = 4, 即该层包含 num_expert 个并列的Expert。\n",
    "\n",
    "记 active_expert = 2, 即计算每个Embedding仅有 active_expert 个Expert 参与计算。\n",
    "\n",
    "### 选通网络计算\n",
    "对于有seq_len=8的数据，如果每个Expert都参与计算每一个Embedding，那么一共需要计算 seq_len*embed_size = 32 次， 这极大的增加了模型计算量，因此我们往往只激活其中的active_experts个Expert，这要求我们对每一个Embedding计算最合适的active_experts个 Expert。\n",
    "\n",
    "对于单个Expert 的原版Transformer来说：\n",
    "\n",
    "$$\n",
    "outputs[0,seq] = FeedForward(inputs[0,seq])\n",
    "$$\n",
    "\n",
    "对于多个Expert的网络：\n",
    "\n",
    "$$\n",
    "outputs[0,seq] = \\sum_{i \\in range(num\\_model)} \\alpha_{i} Expert_{i}(inputs[0,seq])\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_{i} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    1 & Expert_{i}  \\text{is selected} \\\\\n",
    "    0 & Expert_{i}  \\text{is not selected} \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "将$\\{\\alpha_0,\\alpha_1,\\dots,\\alpha_{num_experts-1}\\}$记为向量$\\alpha$:\n",
    "$$\n",
    "outputs[0,seq] = \\alpha \\cdot \\{Expert_i(inputs[0,seq])\\}\n",
    "$$\n",
    "\n",
    "一个选通0,2号Expert的$\\alpha$的例子是$[1,0,1,0]$\n",
    "\n",
    "问题在于如何求得 $\\alpha$, 对于一个Embedding ，我们使用神经网络对每个Expert打分，在根据分数计算$\\alpha$\n",
    "\n",
    "$$\n",
    "score[0,seq] = MLP(inputs[0,seq])  \\\\\n",
    "\\alpha = topK(score[0,seq])\n",
    "$$\n",
    "\n",
    "例如：\n",
    "\n",
    "$$\n",
    "score[0,seq] = [11.32,1.54,14.83,-1.90] \\\\\n",
    "\\alpha = [1,0,1,0]\n",
    "$$\n",
    "\n",
    "从优化的角度来说，$\\alpha$取前k大的分数的下标（即argmax），这个操作是不可导的，这里我们用之前在\"attention.ipynb\"中提到的技巧处理这里的计算。\n",
    "\n",
    "$$\n",
    "mask(score[0,seq]) = [11.32,-inf,14.83,-inf] \\\\\n",
    "\\alpha = softmax(mask(score[0,seq])) = [0.028,0,0.971,0] \\\\\n",
    "index = [1,0,1,0]\n",
    "$$\n",
    "\n",
    "我们用这个$\\alpha$和$index$用做选通网络."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First define the top k router module\n",
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, embed_size, num_experts, active_experts):\n",
    "        ## TODO\n",
    "        super().__init__()\n",
    "        ## embed_size : dimension of embedding \n",
    "        ## num_experts : how many Experts per layer\n",
    "        ## active_experts: only active_experts out of num_experts are selected to process Embeddings per token.\n",
    "        self.k = active_experts\n",
    "        self.linear = nn.Linear(embed_size, num_experts)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## TODO\n",
    "        ## 完成这部分时，注意使用Softmax()对router_output做标准化。同时注意这部分所用操作的可导性。\n",
    "        ## 输入值\n",
    "        ## inputs is the output tensor from multihead self attention block, shape (B:batch size, T: seq_len, C: embed_size)\n",
    "        ## 返回值\n",
    "        ## router_output: normalized weight of Experts, 即教程中的 \\alpha\n",
    "        ## indices:   index of selected Experts, 即教程中的 index\n",
    "        linear_output = self.linear(inputs)\n",
    "        topk, indices = torch.topk(linear_output, self.k, dim=-1)\n",
    "        inf = torch.full_like(linear_output, -float('inf'))\n",
    "        masked = inf.scatter(-1, indices, topk)\n",
    "        router_output = F.softmax(masked, dim=-1)\n",
    "        return router_output, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 稀疏专家网络 SparseMoE\n",
    "\n",
    "![moe](./moeSparse.png)\n",
    "\n",
    "在定义完Expert 和 TopkRouter后，我们可以定义SparseMoE模块。\n",
    "\n",
    "在前向过程中，对于inputs.shape = [Batch_size,seq_len,embed_size]第二维度seq_len个Embedding,我们先利用TopkRouter计算出选通专家序号indices以及专家权重router_output。\n",
    "\n",
    "我们将Embedding通过选通的Expert得出active_expert个新的Embedding，然后使用router_output的作为权重对新的Embedding加权求和作为输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, embed_size:int, num_experts:int, active_experts:int):\n",
    "        ## TODO\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.k = active_experts\n",
    "        self.experts = nn.ModuleList([Expert(embed_size) for _ in range(num_experts)])\n",
    "        self.router = TopkRouter(embed_size, num_experts, active_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## TODO\n",
    "        # input: (batch_size, seq_len, embed_size)\n",
    "        # weight, indices = self.router(inputs) #indices: [2,5,7...]\n",
    "        # outputs = torch.zeros_like(inputs)\n",
    "\n",
    "        # flat_inputs = inputs.view(-1, inputs.size(-1))\n",
    "        # flat_weight = weight.view(-1, weight.size(-1))\n",
    "\n",
    "        # for i in range(len(self.experts)):\n",
    "        #     mask = (indices == i)\n",
    "        #     flag = mask.any()\n",
    "        #     if flag:\n",
    "        #         expert_out = self.experts[i](flat_inputs[flag]) #?\n",
    "        #         we = flat_weight[flag][i].unsqueeze(1)\n",
    "        #         expert_out = expert_out * we\n",
    "        #         outputs[flag] += expert_out.squeeze(1)\n",
    "\n",
    "        gating_output, indices = self.router(x)\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        # Reshape inputs for batch processing\n",
    "        flat_x = x.view(-1, x.size(-1))\n",
    "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
    "\n",
    "        # Process each expert in parallel\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Create a mask for the inputs where the current expert is in top-k\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            flat_mask = expert_mask.view(-1)\n",
    "\n",
    "            if flat_mask.any():\n",
    "                expert_input = flat_x[flat_mask]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                # Extract and apply gating scores\n",
    "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
    "                weighted_output = expert_output * gating_scores\n",
    "\n",
    "                # Update final output additively by indexing and adding\n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer由一层层的block堆叠而成，其中每个block的结构从模型的结构图展开中可以看到，由LayerNorm，Masked multi head attention，(SparseMoE)FeedForward组成。\n",
    "\n",
    "对于一个表示句子的输入向量x，其首先会经过Layer Normalization层.\n",
    "Layer Normalization 层对于一个 句子个数x句子长度x单词向量维度 的输入 x, 会在最后两维上进行规范化处理，起到稳定训练的作用。\n",
    "\n",
    "$$\n",
    "LN(x)=\\frac{x-mean(x)}{\\sqrt{var(x)+\\epsilon}}\\cdot\\gamma+\\beta\n",
    "$$\n",
    "\n",
    "其中mean和var都是在最后两个维度上进行的，layernorm的实现同学们可以直接调用nn.LayerNorm\n",
    "经过layernorm层后，再经过Mask multi head attention层之后，会在+号处再次和原始的输入进行相加，这样的做法能够提高训练的稳定性。有兴趣的同学可以从梯度角度思考原因，或者搜索残差连接相关资料进行学习。\n",
    "之后再同样经过一层layernorm和feedforwad之后，就可以得到block块的输出了。\n",
    "即 x' = x+MHA(LN(x)), y = FFN(LN(x'))+x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    # Transformer basic block, consist of MultiHeadAttention, FeedForward and layer normalization\n",
    "    def __init__(self, embed_size:int, n_heads:int, seq_len:int, num_experts:int, active_experts:int):\n",
    "        super().__init__()\n",
    "        # TODO: implement block structure\n",
    "        self.mha = MultiHeadAttention(n_heads, embed_size//n_heads, seq_len, embed_size)\n",
    "        self.smoe = SparseMoE(embed_size, num_experts, active_experts)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size)\n",
    "        #TODO: forward with residual connection\n",
    "        x = inputs + self.mha(self.ln1(inputs))\n",
    "        x = x + self.smoe(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoETransformer(nn.Module):\n",
    "    # Transformer decoder, consist of \n",
    "    # token embedding layer and position_embedding(position_embedding 可以理解为对位置编码，感兴趣的同学可以查阅原文，这里可以看为vocab_len = seq_len的Embedding)\n",
    "    # a stack of Transformer basic block\n",
    "    # a layernorm and output linear layer\n",
    "    def __init__(self, vocab_size:int, seq_len:int, embed_size:int, n_layers:int, n_heads:int, num_experts:int, active_experts:int):\n",
    "        # vocab_size is the number of word in vocabulary dict\n",
    "        # seq_len is the sequence length/sentence length\n",
    "        # embed_size is the embedding vector dimension\n",
    "        super().__init__()\n",
    "        # TODO: \n",
    "        self.seq_len = seq_len\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(seq_len, embed_size)\n",
    "        self.blocks = nn.Sequential(*[Block(embed_size, n_heads, seq_len, num_experts, active_experts) for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        # labels: the (ground) true output \n",
    "        # TODO: implement the forward function of the transformer\n",
    "\n",
    "        # inputs:(batch_size, seq_len, )\n",
    "        batch_size, seq_len, = inputs.shape\n",
    "        # embedding:(batch_size, seq_len, embed_size)\n",
    "        embedding = self.token_embedding(inputs)\n",
    "        embedding += self.position_embedding(torch.arange(seq_len, device=device))\n",
    "        # attens:(batch_size, seq_len, embed_size)\n",
    "        attens = self.norm(self.blocks(embedding))\n",
    "        # logits:(batch_size, seq_len, vocab_size)\n",
    "        logits = self.linear(attens)\n",
    "\n",
    "        # compute the loss\n",
    "        \n",
    "        if labels is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits = logits.view(batch_size * seq_len, vocab_size)\n",
    "            labels = labels.view(batch_size * seq_len)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        inputs = torch.tensor(tokenizer.encode(inputs)).unsqueeze(0)\n",
    "        device = next(self.parameters()).device  \n",
    "        inputs = inputs.to(device)\n",
    "        if inputs.size(1) > self.seq_len:\n",
    "            inputs = inputs[:, :self.seq_len]\n",
    "        generated = inputs\n",
    "        for _ in range(max_new_tokens):\n",
    "            if generated.size(1) > self.seq_len:\n",
    "                generated_input = generated[:, -self.seq_len:]\n",
    "            else:\n",
    "                generated_input = generated\n",
    "            logits, _ = self.forward(generated_input)\n",
    "            last_logits = logits[:, -1, :]  \n",
    "            next_token_ids = torch.argmax(last_logits, dim=-1)  \n",
    "            next_token_ids = next_token_ids.unsqueeze(-1)  \n",
    "            generated = torch.cat([generated, next_token_ids], dim=1)  \n",
    "        return generated\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练循环\n",
    "\n",
    "如果你已经完成了模型定义等内容，训练的过程实际上在高度封装的 Pytorch 库中非常简单, 因为你并不需要写对应的反向传播。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss \n",
    "\n",
    "Loss 用来**衡量**模型预测与真实值之间的**差距**。\n",
    "\n",
    "常见的几个 Loss 函数：\n",
    "\n",
    "* 交叉熵：$\\text{CrossEntropy Loss} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)$\n",
    "* 均方误差：$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$\n",
    "* 绝对误差：$\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y_i}|$\n",
    "\n",
    "不同的 loss 对应不同的优化目标，如果写错 loss 函数会导致模型不收敛/性能很差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练循环\n",
    "\n",
    "当我们写好 Optimizer 和 Loss 之后，对应的训练循环就十分简单了。\n",
    "\n",
    "我们只需要做以下事情：\n",
    "\n",
    "* 从 dataloader 里面拿到一个 batch 的数据以及标签\n",
    "* 将数据送入模型，进行前向传播\n",
    "* 拿到模型输出的 logits\n",
    "* 将 logits 和 标签进行 loss 计算\n",
    "* 用 Optimizer \n",
    "    * 清空梯度\n",
    "    * 反向传播\n",
    "    * 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epoch, device):\n",
    "    # Optimizer 会根据模型的输出和真实标签计算梯度，然后利用反向传播算法更新模型的参数。\n",
    "    # 在本实验中你可以将 Optimizer 视作黑盒，只需要知道如何使用即可。\n",
    "    # 找一个合适的 Optimizer。对不同的任务，模型，最适合的优化器是不一样的，你可以先尝试最常用的 Adam，如果有兴趣可以看看其他的优化器。\n",
    "    # docs see: https://pytorch.org/docs/stable/optim.html \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    from tqdm import tqdm\n",
    "    for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # TODO: implement the training process, and compute the training loss and validation loss\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch} Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, epoch, device):\n",
    "    model.eval()\n",
    "    # TODO: 实现验证函数。与训练函数类似，但不需要计算梯度。\n",
    "    total_loss = 0\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits, loss = model(inputs, targets)\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch} Validation Loss: {total_loss / len(dataloader)}')\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1743 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [02:22<00:00, 12.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 1.7497408216326522\n",
      "Epoch 0 Validation Loss: 1.392071222493408\n",
      "Epoch 0 Train Loss: 1.7497408216326522, Valid Loss: 1.392071222493408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [02:21<00:00, 12.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.3363439250252265\n",
      "Epoch 1 Validation Loss: 1.2971134664268669\n",
      "Epoch 1 Train Loss: 1.3363439250252265, Valid Loss: 1.2971134664268669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [02:22<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.2695942013276014\n",
      "Epoch 2 Validation Loss: 1.250436696164105\n",
      "Epoch 2 Train Loss: 1.2695942013276014, Valid Loss: 1.250436696164105\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309931/1611702591.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(tokenizer.encode(inputs)).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could pick my lance is not to the sea\n",
      "Of some of the sea to be a sentence of the sea\n",
      "And see the sea to make a sentence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader = create_dataloader('input.txt', tokenizer, chunk_size=50, batch_size=512)\n",
    "model = SparseMoETransformer(vocab_size=128, seq_len=50, embed_size=64, n_layers=3, n_heads=8, num_experts=8, active_experts=2).to(device)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 训练模型\n",
    "train_list = []\n",
    "valid_list = []\n",
    "def run(model, train_dataloader, valid_dataloader, device, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model, train_dataloader, epoch, device)\n",
    "        valid_loss = validate(model, valid_dataloader, epoch, device)\n",
    "        print(f'Epoch {epoch} Train Loss: {train_loss}, Valid Loss: {valid_loss}')\n",
    "        train_list.append(train_loss)\n",
    "        valid_list.append(valid_loss)\n",
    "    plt.plot(train_list, label='train')\n",
    "    plt.plot(valid_list, label='valid')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig('loss.png')\n",
    "\n",
    "#TODO: 用 matplotlib plot 训练过程中的 loss 变化\n",
    "\n",
    "\n",
    "# run(model, dataloader[0], None, device, epochs=100)\n",
    "run(model, dataloader[0], dataloader[1], device, epochs=10)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "print(tokenizer.decode(model.generate(\"I could pick my lance\",max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309931/1611702591.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(tokenizer.encode(inputs)).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could pick my lance is not to the sea\n",
      "Of some of the sea to be a sentence of the sea\n",
      "And see the sea to make a sentence\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "print(tokenizer.decode(model.generate(\"I could pick my lance\",max_new_tokens=100)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

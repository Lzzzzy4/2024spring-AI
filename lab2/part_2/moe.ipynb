{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List\n",
    "import tiktoken\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### 简介\n",
    "Tokenization 的主要目的是将文本分解成更小的单位(Tokens)，减小模型输入数据的内在结构复杂度(从句子变为单词序列)，从而简化模型训练的难度。同时将字符的序列转化为Token序号的序列，便于模型输入。\n",
    "\n",
    "Tokenization 首先确定语言的词表划分粒度，一般可分为：\n",
    "* 字符级：将文本分解为字符。\n",
    "* 单词级：将文本分解为单词。\n",
    "* 子词级：将单词进一步分解为更小的有意义单元（如前缀、后缀）。\n",
    "\n",
    "之后使用预定义的规则来识别 tokens, 或使用统计或机器学习技术来识别最优的 token 切分方式。例如，BPE（Byte Pair Encoding）或 SentencePiece。\n",
    "\n",
    "最后实现一组文本序列和Tokens序列之间相互转化的函数，即可完成Tokenization部分。\n",
    "\n",
    "### 实验要求\n",
    "\n",
    "1. 实现字符级切分的简单tokenizer， 由 字符表， 字符到token的 encoder()函数 和 token到字符的 decoder() 函数组成。\n",
    "2. 调用 现有的tokenizer实现，比如openai 的tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataPath:str\n",
    "        ):\n",
    "        with open(dataPath,\"r\",encoding=\"utf-8\") as f:\n",
    "            self.dataset = f.read()\n",
    "        self.generate_vocabulary()\n",
    "\n",
    "    def generate_vocabulary(\n",
    "        self,\n",
    "        ):\n",
    "        # self.char2index = {\n",
    "        #     'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'H':8, 'I':9, 'J':10,\n",
    "        #     'K':11, 'L':12, 'M':13, 'N':14, 'O':15, 'P':16, 'Q':17, 'R':18, 'S':19,\n",
    "        #     'T':20, 'U':21, 'V':22, 'W':23, 'X':24, 'Y':25, 'Z':26, 'a':27, 'b':28,\n",
    "        #     'c':29, 'd':30, 'e':31, 'f':32, 'g':33, 'h':34, 'i':35, 'j':36, 'k':37,\n",
    "        #     'l':38, 'm':39, 'n':40, 'o':41, 'p':42, 'q':43, 'r':44, 's':45, 't':46,\n",
    "        #     'u':47, 'v':48, 'w':49, 'x':50, 'y':51, 'z':52, ' ':53, ',':54, '.':55,\n",
    "        #     '!':56, '?':57, ':':58, ';':59, \"'\":60, '\"':61, '0':62, '1':63, '2':64,\n",
    "        #     }\n",
    "        # self.index2char = {1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F', 7: 'G', 8: 'H', 9: 'I', 10: 'J', 11: 'K', 12: 'L', 13: 'M', 14: 'N', 15: 'O', 16: 'P', 17: 'Q', 18: 'R', 19: 'S', 20: 'T', 21: 'U', 22: 'V', 23: 'W', 24: 'X', 25: 'Y', 26: 'Z', 27: ' ', 28: 'a', 29: 'b', 30: 'c', 31: 'd', 32: 'e', 33: 'f', 34: 'g', 35: 'h', 36: 'i', 37: 'j', 38: 'k', 39: 'l', 40: 'm', 41: 'n', 42: 'o', 43: 'p', 44: 'q', 45: 'r', 46: 's', 47: 't', 48: 'u', 49: 'v', 50: 'w', 51: 'x', 52: 'y', 53: 'z'}\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        sentence : str,\n",
    "        ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        例子, 假设A-Z 对应的token是1-26, 句子开始，结束符号的token是0。\n",
    "        input  : \"ABCD\"\n",
    "        output : Tensor([0,1,2,3]) \n",
    "        注意: 为了后续实验方便，输出Tensor的数据类型dtype 为torch.long。\n",
    "        \"\"\"\n",
    "        # for char in sentence:\n",
    "        #     if char not in self.char2index:\n",
    "        #         raise ValueError(\"Invalid character in sentence\")\n",
    "        ans = torch.tensor([ord(char) for char in sentence],dtype=torch.long)\n",
    "        ans = torch.cat([torch.tensor([0],dtype=torch.long),ans])\n",
    "        return ans\n",
    "    \n",
    "    def decode(\n",
    "        self,\n",
    "        tokens : torch.Tensor,\n",
    "        ) -> str:\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        例子, 假设A-Z 对应的token是1-26, 句子开始，结束符号的token是0。\n",
    "        input : Tensor([0,1,2,3]) \n",
    "        output : \"ABCD\"\n",
    "        \"\"\"\n",
    "        ans = \"\"\n",
    "        for token in tokens:\n",
    "            if token == 0:\n",
    "                continue\n",
    "            ans += chr(token)\n",
    "        return ans\n",
    "\n",
    "# enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "# print(enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 dataloader 和 dataset\n",
    "\n",
    "为了高效加载数据，我们需要把输入文件接入 PyTorch 的数据加载器中。在这里我们定义 `ShakespeareDataset` 类用于加载数据集，用 PyTorch 的 `DataLoader` 类来实现数据加载。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, filepath, tokenizer, chunk_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        self.encoded = self.tokenizer.encode(text)\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded) - self.chunk_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: 提取一段文本(长度为 chunk_size）作为输入，以及这段文本的每一个字符的下一个字符作为标签\n",
    "        # example(not correspond to real text): chunk = tensor([ 0, 20, 49, 58, 59])\n",
    "        #         label = tensor([20, 49, 58, 59, 19])\n",
    "        # decoded chunk: \"The \"\n",
    "        # decoded label: \"he T\"\n",
    "        chunk = self.encoded[idx:idx+self.chunk_size]\n",
    "        label = self.encoded[idx+1:idx+self.chunk_size+1]\n",
    "        return chunk, label\n",
    "\n",
    "tokenizer = Tokenizer(dataPath=\"input.txt\")\n",
    "\n",
    "def create_dataloader(filepath, tokenizer, chunk_size, batch_size, shuffle=True):\n",
    "    dataset = ShakespeareDataset(filepath, tokenizer, chunk_size)\n",
    "    train_dataset,val_dataset = torch.utils.data.random_split(dataset,[int(len(dataset)*0.8),len(dataset)-int(len(dataset)*0.8)])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "train_dataloader,val_dataloader = create_dataloader('input.txt', tokenizer, chunk_size=200, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意力的计算公式为：\n",
    "$$\n",
    "Head = Attention(x)=Softmax(M\\cdot QK^T)V\\\\\n",
    "Q=xW_{q},K=xW_{k}, V=xW_{v}\n",
    "$$\n",
    "这里实现的一些数学技巧可以参见attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadAttention(nn.Module):\n",
    "    def __init__(self, seq_len:int, embed_size:int, hidden_size:int):\n",
    "        super().__init__()\n",
    "        # embed_size: dimension for input embedding vector\n",
    "        # hidden_size: dimension for hidden vector. eg. x:(..., embed_size) --to_q--> query_vector:(..., hidden_size)\n",
    "\n",
    "        # a triangular bool matrix for mask\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(seq_len, seq_len)))\n",
    "        \n",
    "        # TODO: init three matrix, to_q, to_k, to_v.\n",
    "        self.to_q = nn.Linear(embed_size, hidden_size, bias=False) # hidden_size ?\n",
    "        self.to_k = nn.Linear(embed_size, hidden_size, bias=False)\n",
    "        self.to_v = nn.Linear(embed_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size)\n",
    "        # return (batch_size, seq_len, hidden_size)\n",
    "        # TODO: implement the attention mechanism\n",
    "        B, T, C = inputs.shape\n",
    "        # 1. get q, k, v\n",
    "        q = self.to_q(inputs) # (batch_size, seq_len, hidden_size)\n",
    "        k = self.to_k(inputs)\n",
    "        v = self.to_v(inputs)\n",
    "        # 2. calculate attention score\n",
    "        Score = q @ k.transpose(-1,-2)\n",
    "        Score = Score / (C ** 0.5)\n",
    "        # 3. calculate attention score with mask\n",
    "        Score = Score.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        # 4. calculate softmax\n",
    "        Score = F.softmax(Score, dim=-1)\n",
    "        # 5. calculate output\n",
    "        output = Score @ v\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer中使用的注意力机制时会使用多个注意力头，期望每个注意力头能够注意到不同的信息。\n",
    "所以实际公式需要修改如下\n",
    "$$\n",
    "MultiHeadAttention(x)=[Head_0, Head_1,...,Head_h]W_o\\\\\n",
    "Head_i = Attention(x)=Softmax(M\\cdot Q_iK_i^T)V_i\\\\\n",
    "Q_i=xW_{iq},K=xW_{ik}, V=xW_{iv}\n",
    "$$\n",
    "在搭建网络的过程中，同学们可能会用到nn.ModuleList这个库，每个$Head_i$的计算可以直接使用上面已经实现的单头注意力计算。\n",
    "最后对于这些注意力头再使用一个简单的线性层/矩阵$W_o$汇总信息即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # MultiHeadAttention is consist of many HeadAttention output.\n",
    "    # concat all this head attention output o_i, then merge them with a projection matrix W_o, as [o_1, o_2, ...] x W_o\n",
    "    # The reason for using multi-head attention is that we want each head to be able to extract different features\n",
    "    def __init__(self, n_heads:int, head_size:int, seq_len:int, embed_size:int):\n",
    "        # n_heads is the number of head attention\n",
    "        # head_size is the hidden_size in each HeadAttention\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        #TODO: implement heads and projection\n",
    "        self.heads = nn.ModuleList([HeadAttention(seq_len, embed_size, head_size) for _ in range(n_heads)])\n",
    "        self.projection = nn.Linear(embed_size, embed_size, bias=False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size), make sure embed_size=n_heads x head_size\n",
    "        # return: (batch_size, seq_len, embed_size)\n",
    "        # TODO:\n",
    "        outputs = [head(inputs) for head in self.heads]\n",
    "        outputs = torch.cat(outputs, dim=-1)\n",
    "        outputs = self.projection(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 专家网络 Expert\n",
    "\n",
    "Expert即为标准Transformer中的FeedForward模块。\n",
    "\n",
    "在经过MultiHeadAttention 模块后，seq_len中的每一个Embedding都对应了前文信息的加权求和。在经过FeedForward模块时，模型对每一个位置的Embedding进行了两次线性变换和一次非线性变换，可以视为对当前语境下的信息进行加工。知识编辑的一些研究表明，FeedForword 模块参数包含了大量的事实性知识。\n",
    "\n",
    "一个直观的想法是，类比于MultiHeadAttention，我们在每一层训练多个FeedForward模块，对于不同位置的Embedding使用不同的FeedForward模块处理对应的信息。就好像每层有多个Expert,每个Expert都负责处理一类数据的深加工，因此我们称FeedForward为Expert。\n",
    "\n",
    "实现方面:\n",
    "\n",
    "FeedForward层由两层简单的线性层组成，对于一个(batch_size, seq_len, embed_size)输入的向量x\n",
    "只在最后一个维度上进行计算，以实现词的特征维度上的交互(注意力机制是词之间的交互)。\n",
    "其首先用一个线性层将x最后一维扩大至原先4倍，然后继续用一个线性层还原回原先的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, embed_size:int):\n",
    "        super().__init__()\n",
    "        #TODO: init two linear layer\n",
    "        self.linear1 = nn.Linear(embed_size, 4*embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(4*embed_size, embed_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: (batch_size, seq_len, embed_size)\n",
    "        # -> mid: (batch_size, seq_len, 4 x embed_size)\n",
    "        # -> outputs: (batch_size, seq_len, embed_size)\n",
    "        return self.linear2(self.relu(self.linear1(inputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选通网络 TopkRouter\n",
    "\n",
    "在实现了单个Expert后，我们要设计一个选通网络决策每个Embedding要使用那个Expert计算\n",
    "\n",
    "\n",
    "### 为了说明选通网络的实现方式，我们定义一下记号：\n",
    "\n",
    "inputs.shape = [batch_size, seq_len, embed_size] = [1, 8, 16] \n",
    "\n",
    "即输入有batch_size=1个数据点，该数据有seq_len长度的context，即包含seq_len=8个Embedding，每个Embedding长度为embed_dim=16。\n",
    "\n",
    "记 num_expert = 4, 即该层包含 num_expert 个并列的Expert。\n",
    "\n",
    "记 active_expert = 2, 即计算每个Embedding仅有 active_expert 个Expert 参与计算。\n",
    "\n",
    "### 选通网络计算\n",
    "对于有seq_len=8的数据，如果每个Expert都参与计算每一个Embedding，那么一共需要计算 seq_len*embed_size = 32 次， 这极大的增加了模型计算量，因此我们往往只激活其中的active_experts个Expert，这要求我们对每一个Embedding计算最合适的active_experts个 Expert。\n",
    "\n",
    "对于单个Expert 的原版Transformer来说：\n",
    "\n",
    "$$\n",
    "outputs[0,seq] = FeedForward(inputs[0,seq])\n",
    "$$\n",
    "\n",
    "对于多个Expert的网络：\n",
    "\n",
    "$$\n",
    "outputs[0,seq] = \\sum_{i \\in range(num\\_model)} \\alpha_{i} Expert_{i}(inputs[0,seq])\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_{i} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    1 & Expert_{i}  \\text{is selected} \\\\\n",
    "    0 & Expert_{i}  \\text{is not selected} \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "将$\\{\\alpha_0,\\alpha_1,\\dots,\\alpha_{num_experts-1}\\}$记为向量$\\alpha$:\n",
    "$$\n",
    "outputs[0,seq] = \\alpha \\cdot \\{Expert_i(inputs[0,seq])\\}\n",
    "$$\n",
    "\n",
    "一个选通0,2号Expert的$\\alpha$的例子是$[1,0,1,0]$\n",
    "\n",
    "问题在于如何求得 $\\alpha$, 对于一个Embedding ，我们使用神经网络对每个Expert打分，在根据分数计算$\\alpha$\n",
    "\n",
    "$$\n",
    "score[0,seq] = MLP(inputs[0,seq])  \\\\\n",
    "\\alpha = topK(score[0,seq])\n",
    "$$\n",
    "\n",
    "例如：\n",
    "\n",
    "$$\n",
    "score[0,seq] = [11.32,1.54,14.83,-1.90] \\\\\n",
    "\\alpha = [1,0,1,0]\n",
    "$$\n",
    "\n",
    "从优化的角度来说，$\\alpha$取前k大的分数的下标（即argmax），这个操作是不可导的，这里我们用之前在\"attention.ipynb\"中提到的技巧处理这里的计算。\n",
    "\n",
    "$$\n",
    "mask(score[0,seq]) = [11.32,-inf,14.83,-inf] \\\\\n",
    "\\alpha = softmax(mask(score[0,seq])) = [0.028,0,0.971,0] \\\\\n",
    "index = [1,0,1,0]\n",
    "$$\n",
    "\n",
    "我们用这个$\\alpha$和$index$用做选通网络."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First define the top k router module\n",
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, embed_size, num_experts, active_experts):\n",
    "        ## TODO\n",
    "        super().__init__()\n",
    "        ## embed_size : dimension of embedding \n",
    "        ## num_experts : how many Experts per layer\n",
    "        ## active_experts: only active_experts out of num_experts are selected to process Embeddings per token.\n",
    "        self.k = active_experts\n",
    "        self.linear = nn.Linear(embed_size, num_experts)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## TODO\n",
    "        ## 完成这部分时，注意使用Softmax()对router_output做标准化。同时注意这部分所用操作的可导性。\n",
    "        ## 输入值\n",
    "        ## inputs is the output tensor from multihead self attention block, shape (B:batch size, T: seq_len, C: embed_size)\n",
    "        ## 返回值\n",
    "        ## router_output: normalized weight of Experts, 即教程中的 \\alpha\n",
    "        ## indices:   index of selected Experts, 即教程中的 index\n",
    "        linear_output = self.linear(inputs)\n",
    "        topk, indices = torch.topk(linear_output, self.k, dim=-1)\n",
    "        inf = torch.full_like(linear_output, -float('inf'))\n",
    "        masked = inf.scatter(-1, indices, topk)\n",
    "        router_output = F.softmax(masked, dim=-1)\n",
    "        return router_output, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 稀疏专家网络 SparseMoE\n",
    "\n",
    "![moe](./moeSparse.png)\n",
    "\n",
    "在定义完Expert 和 TopkRouter后，我们可以定义SparseMoE模块。\n",
    "\n",
    "在前向过程中，对于inputs.shape = [Batch_size,seq_len,embed_size]第二维度seq_len个Embedding,我们先利用TopkRouter计算出选通专家序号indices以及专家权重router_output。\n",
    "\n",
    "我们将Embedding通过选通的Expert得出active_expert个新的Embedding，然后使用router_output的作为权重对新的Embedding加权求和作为输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, embed_size:int, num_experts:int, active_experts:int):\n",
    "        ## TODO\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.k = active_experts\n",
    "        self.experts = nn.ModuleList([Expert(embed_size) for _ in range(num_experts)])\n",
    "        self.router = TopkRouter(embed_size, num_experts, active_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## TODO\n",
    "        weight, indices = self.router(x)\n",
    "        flat_x = x.view(-1, x.size(-1))\n",
    "        flat_weight = weight.view(-1, weight.size(-1))\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        for i in range(len(self.experts)):\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            mask = expert_mask.view(-1)\n",
    "            if mask.any():\n",
    "                expert_input = flat_x[mask]\n",
    "                expert_output = self.experts[i](expert_input)\n",
    "                gating_scores = flat_weight[mask, i].unsqueeze(1)\n",
    "                weighted_output = expert_output * gating_scores\n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer由一层层的block堆叠而成，其中每个block的结构从模型的结构图展开中可以看到，由LayerNorm，Masked multi head attention，(SparseMoE)FeedForward组成。\n",
    "\n",
    "对于一个表示句子的输入向量x，其首先会经过Layer Normalization层.\n",
    "Layer Normalization 层对于一个 句子个数x句子长度x单词向量维度 的输入 x, 会在最后两维上进行规范化处理，起到稳定训练的作用。\n",
    "\n",
    "$$\n",
    "LN(x)=\\frac{x-mean(x)}{\\sqrt{var(x)+\\epsilon}}\\cdot\\gamma+\\beta\n",
    "$$\n",
    "\n",
    "其中mean和var都是在最后两个维度上进行的，layernorm的实现同学们可以直接调用nn.LayerNorm\n",
    "经过layernorm层后，再经过Mask multi head attention层之后，会在+号处再次和原始的输入进行相加，这样的做法能够提高训练的稳定性。有兴趣的同学可以从梯度角度思考原因，或者搜索残差连接相关资料进行学习。\n",
    "之后再同样经过一层layernorm和feedforwad之后，就可以得到block块的输出了。\n",
    "即 x' = x+MHA(LN(x)), y = FFN(LN(x'))+x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    # Transformer basic block, consist of MultiHeadAttention, FeedForward and layer normalization\n",
    "    def __init__(self, embed_size:int, n_heads:int, seq_len:int, num_experts:int, active_experts:int):\n",
    "        super().__init__()\n",
    "        # TODO: implement block structure\n",
    "        self.mha = MultiHeadAttention(n_heads, embed_size//n_heads, seq_len, embed_size)\n",
    "        self.smoe = SparseMoE(embed_size, num_experts, active_experts)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size)\n",
    "        #TODO: forward with residual connection\n",
    "        x = inputs + self.mha(self.ln1(inputs))\n",
    "        x = x + self.smoe(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoETransformer(nn.Module):\n",
    "    # Transformer decoder, consist of \n",
    "    # token embedding layer and position_embedding(position_embedding 可以理解为对位置编码，感兴趣的同学可以查阅原文，这里可以看为vocab_len = seq_len的Embedding)\n",
    "    # a stack of Transformer basic block\n",
    "    # a layernorm and output linear layer\n",
    "    def __init__(self, vocab_size:int, seq_len:int, embed_size:int, n_layers:int, n_heads:int, num_experts:int, active_experts:int):\n",
    "        # vocab_size is the number of word in vocabulary dict\n",
    "        # seq_len is the sequence length/sentence length\n",
    "        # embed_size is the embedding vector dimension\n",
    "        super().__init__()\n",
    "        # TODO: \n",
    "        self.seq_len = seq_len\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(seq_len, embed_size)\n",
    "        self.blocks = nn.Sequential(*[Block(embed_size, n_heads, seq_len, num_experts, active_experts) for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        # labels: the (ground) true output \n",
    "        # TODO: implement the forward function of the transformer\n",
    "\n",
    "        # inputs:(batch_size, seq_len, )\n",
    "        batch_size, seq_len, = inputs.shape\n",
    "        # embedding:(batch_size, seq_len, embed_size)\n",
    "        embedding = self.token_embedding(inputs)\n",
    "        embedding += self.position_embedding(torch.arange(seq_len, device=device))\n",
    "        # attens:(batch_size, seq_len, embed_size)\n",
    "        attens = self.norm(self.blocks(embedding))\n",
    "        # logits:(batch_size, seq_len, vocab_size)\n",
    "        logits = self.linear(attens)\n",
    "\n",
    "        # compute the loss\n",
    "        \n",
    "        if labels is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits = logits.view(batch_size * seq_len, vocab_size)\n",
    "            labels = labels.view(batch_size * seq_len)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        inputs = torch.tensor(tokenizer.encode(inputs)).unsqueeze(0)\n",
    "        device = next(self.parameters()).device  \n",
    "        inputs = inputs.to(device)\n",
    "        if inputs.size(1) > self.seq_len:\n",
    "            inputs = inputs[:, :self.seq_len]\n",
    "        generated = inputs\n",
    "        for _ in range(max_new_tokens):\n",
    "            if generated.size(1) > self.seq_len:\n",
    "                generated_input = generated[:, -self.seq_len:]\n",
    "            else:\n",
    "                generated_input = generated\n",
    "            logits, _ = self.forward(generated_input)\n",
    "            last_logits = logits[:, -1, :]  \n",
    "            next_token_ids = torch.argmax(last_logits, dim=-1)  \n",
    "            next_token_ids = next_token_ids.unsqueeze(-1)  \n",
    "            generated = torch.cat([generated, next_token_ids], dim=1)  \n",
    "        return generated\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练循环\n",
    "\n",
    "如果你已经完成了模型定义等内容，训练的过程实际上在高度封装的 Pytorch 库中非常简单, 因为你并不需要写对应的反向传播。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss \n",
    "\n",
    "Loss 用来**衡量**模型预测与真实值之间的**差距**。\n",
    "\n",
    "常见的几个 Loss 函数：\n",
    "\n",
    "* 交叉熵：$\\text{CrossEntropy Loss} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)$\n",
    "* 均方误差：$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$\n",
    "* 绝对误差：$\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y_i}|$\n",
    "\n",
    "不同的 loss 对应不同的优化目标，如果写错 loss 函数会导致模型不收敛/性能很差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练循环\n",
    "\n",
    "当我们写好 Optimizer 和 Loss 之后，对应的训练循环就十分简单了。\n",
    "\n",
    "我们只需要做以下事情：\n",
    "\n",
    "* 从 dataloader 里面拿到一个 batch 的数据以及标签\n",
    "* 将数据送入模型，进行前向传播\n",
    "* 拿到模型输出的 logits\n",
    "* 将 logits 和 标签进行 loss 计算\n",
    "* 用 Optimizer \n",
    "    * 清空梯度\n",
    "    * 反向传播\n",
    "    * 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epoch, device):\n",
    "    # Optimizer 会根据模型的输出和真实标签计算梯度，然后利用反向传播算法更新模型的参数。\n",
    "    # 在本实验中你可以将 Optimizer 视作黑盒，只需要知道如何使用即可。\n",
    "    # 找一个合适的 Optimizer。对不同的任务，模型，最适合的优化器是不一样的，你可以先尝试最常用的 Adam，如果有兴趣可以看看其他的优化器。\n",
    "    # docs see: https://pytorch.org/docs/stable/optim.html \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    from tqdm import tqdm\n",
    "    for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # TODO: implement the training process, and compute the training loss and validation loss\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch} Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, epoch, device):\n",
    "    model.eval()\n",
    "    # TODO: 实现验证函数。与训练函数类似，但不需要计算梯度。\n",
    "    total_loss = 0\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits, loss = model(inputs, targets)\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch} Validation Loss: {total_loss / len(dataloader)}')\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 1743/1743 [02:19<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 1.7681570779436016\n",
      "Epoch 0 Validation Loss: 1.413167176990334\n",
      "Epoch 0 Train Loss: 1.7681570779436016, Valid Loss: 1.413167176990334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [02:19<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.3604844679315407\n",
      "Epoch 1 Validation Loss: 1.3209078754853765\n",
      "Epoch 1 Train Loss: 1.3604844679315407, Valid Loss: 1.3209078754853765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [02:20<00:00, 12.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.2951552317324413\n",
      "Epoch 2 Validation Loss: 1.27587812865546\n",
      "Epoch 2 Train Loss: 1.2951552317324413, Valid Loss: 1.27587812865546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [02:20<00:00, 12.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.2551319322843217\n",
      "Epoch 3 Validation Loss: 1.2406299745817797\n",
      "Epoch 3 Train Loss: 1.2551319322843217, Valid Loss: 1.2406299745817797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [02:18<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 1.2243017253011244\n",
      "Epoch 4 Validation Loss: 1.212595626028306\n",
      "Epoch 4 Train Loss: 1.2243017253011244, Valid Loss: 1.212595626028306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [02:15<00:00, 12.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 1.1984459966745995\n",
      "Epoch 5 Validation Loss: 1.1898401155384308\n",
      "Epoch 5 Train Loss: 1.1984459966745995, Valid Loss: 1.1898401155384308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [02:15<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 1.1765770973040328\n",
      "Epoch 6 Validation Loss: 1.1725684453588012\n",
      "Epoch 6 Train Loss: 1.1765770973040328, Valid Loss: 1.1725684453588012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [02:16<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 1.158131600862526\n",
      "Epoch 7 Validation Loss: 1.152341178524385\n",
      "Epoch 7 Train Loss: 1.158131600862526, Valid Loss: 1.152341178524385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [02:17<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 1.141735058164569\n",
      "Epoch 8 Validation Loss: 1.1393561819824605\n",
      "Epoch 8 Train Loss: 1.141735058164569, Valid Loss: 1.1393561819824605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [02:16<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 1.1265002686746217\n",
      "Epoch 9 Validation Loss: 1.1232890494919698\n",
      "Epoch 9 Train Loss: 1.1265002686746217, Valid Loss: 1.1232890494919698\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_332051/1611702591.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(tokenizer.encode(inputs)).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could pick my lance\n",
      "That the state of the world will be so much\n",
      "To seek to the senate, and the rest, the county strengt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader = create_dataloader('input.txt', tokenizer, chunk_size=50, batch_size=512)\n",
    "model = SparseMoETransformer(vocab_size=128, seq_len=50, embed_size=64, n_layers=3, n_heads=8, num_experts=8, active_experts=2).to(device)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 训练模型\n",
    "train_list = []\n",
    "valid_list = []\n",
    "def run(model, train_dataloader, valid_dataloader, device, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model, train_dataloader, epoch, device)\n",
    "        valid_loss = validate(model, valid_dataloader, epoch, device)\n",
    "        print(f'Epoch {epoch} Train Loss: {train_loss}, Valid Loss: {valid_loss}')\n",
    "        train_list.append(train_loss)\n",
    "        valid_list.append(valid_loss)\n",
    "    plt.plot(train_list, label='train')\n",
    "    plt.plot(valid_list, label='valid')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig('loss.png')\n",
    "\n",
    "#TODO: 用 matplotlib plot 训练过程中的 loss 变化\n",
    "\n",
    "\n",
    "# run(model, dataloader[0], None, device, epochs=100)\n",
    "run(model, dataloader[0], dataloader[1], device, epochs=10)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "print(tokenizer.decode(model.generate(\"I could pick my lance\",max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_332051/1611702591.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(tokenizer.encode(inputs)).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could pick my lance\n",
      "That the state of the world will be so much\n",
      "To seek to the senate, and the rest, the county strengt\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "print(tokenizer.decode(model.generate(\"I could pick my lance\",max_new_tokens=100)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
